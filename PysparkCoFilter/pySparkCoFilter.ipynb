{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKhCJtCoV8iV",
        "outputId": "c539ad5d-32d3-4eed-964f-d23c4fe67943"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "gYteAOUTJdIh"
      },
      "outputs": [],
      "source": [
        "# !apt-get -q install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -qnc https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n",
        "# !tar xf spark-3.2.1-bin-hadoop2.7.tgz\n",
        "# !pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "IeaNBlEuLr2i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import findspark\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/home/shane/Co_filter_code/spark-3.2.1-bin-hadoop2.7\"\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0dyoMOAj07p"
      },
      "source": [
        "## [Reference CoFilter With Spark](https://spark.apache.org/docs/2.3.0/ml-collaborative-filtering.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "haHceADeVuwO"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession, Row\n",
        "\n",
        "from pyspark.sql.types import StructType,StructField,IntegerType,StringType,LongType\n",
        "from pyspark.sql.functions import col, conv, explode, collect_list, desc, sort_array, rank\n",
        "from pyspark.ml.recommendation import ALS\n",
        "# from pyspark.ml.evaluation import RegressionMetrics, RankingMetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "citQP33VTwJP",
        "outputId": "f1e88d80-3fdb-4c9a-baf3-53718e190d58"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/04/12 05:50:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , customer_id, article_id, rating\n",
            " Schema: _c0, customer_id, article_id, rating\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///home/shane/Co_filter_code/co_filter/sampled_1000_user_all_purchase.csv\n",
            "22/04/12 05:50:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , customer_id, article_id, rating\n",
            " Schema: _c0, customer_id, article_id, rating\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///home/shane/Co_filter_code/co_filter/sampled_1000_user_all_purchase.csv\n",
            "22/04/12 05:50:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , customer_id, article_id, rating\n",
            " Schema: _c0, customer_id, article_id, rating\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///home/shane/Co_filter_code/co_filter/sampled_1000_user_all_purchase.csv\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "userCount = \"all\"\n",
        "\n",
        "ss = SparkSession.builder.appName('Colab').getOrCreate()\n",
        "inputParsed = ss.read.options(header=True, inferSchema='True',delimiter=',').csv(f\"./co_filter/sampled_{userCount}_user_all_purchase.csv\").rdd\n",
        "ratingRDD = inputParsed.map(lambda p: Row(customerId=int(p[1]), articleId=int(p[2]), count=int(p[3])))\n",
        "ratingDf = ss.createDataFrame(ratingRDD)\n",
        "ratingMean = ratingDf.groupby('customerId').mean(\"count\")\n",
        "ratingDf = ratingDf.join(ratingMean, on=\"customerId\", how=\"left\")\n",
        "ratingDf = ratingDf.withColumn(\"countNorm\", ratingDf[\"count\"] - ratingDf[\"avg(count)\"])\n",
        "# ratingDf.show(5)\n",
        "\n",
        "import pickle\n",
        "def pickle_dump(path, d):\n",
        "    with open(path, 'wb+') as f:\n",
        "        pickle.dump(d, f)\n",
        "pickle_dump(f\"./co_filter/ratings_averaged_{userCount}_user.pkl\", ratingDf.collect())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "7zCqpRUShnzZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# predictions = model.transform(test)\n",
        "# evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
        "#                                 predictionCol=\"prediction\")\n",
        "# rmse = evaluator.evaluate(predictions)\n",
        "\n",
        "# userRecs = model.recommendForAllUsers(3)\n",
        "# evaluator = RegressionMetrics()\n",
        "# rmse = evaluator.evaluate(predictions)\n",
        "# print(\"Root-mean-square error = \" + str(rmse))\n",
        "# userRecs.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "YsMRdQgMGk92"
      },
      "outputs": [],
      "source": [
        "# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "# from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# param_grid = ParamGridBuilder() \\\n",
        "#             .addGrid(als.rank, [10, 50, 100, 150]) \\\n",
        "#             .addGrid(als.regParam, [.01, .05, .1, .15]) \\\n",
        "#             .build()\n",
        "\n",
        "# # Define evaluator as RMSE and print length of evaluator\n",
        "# evaluator = RegressionEvaluator(\n",
        "#            metricName=\"rmse\", \n",
        "#            labelCol=\"countNorm\", \n",
        "#            predictionCol=\"prediction\") \n",
        "# print (\"Num models to be tested: \", len(param_grid))\n",
        "\n",
        "\n",
        "# # Build cross validation using CrossValidator\n",
        "# cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
        "\n",
        "# #Fit cross validator to the 'train' dataset\n",
        "# model = cv.fit(train)\n",
        "# #Extract best model from the cv model above\n",
        "# best_model = model.bestModel\n",
        "# # View the predictions\n",
        "# test_predictions = best_model.transform(test)\n",
        "# RMSE = evaluator.evaluate(test_predictions)\n",
        "# print(RMSE)\n",
        "\n",
        "# print(\"**Best Model**\")\n",
        "# # Print \"Rank\"\n",
        "# print(\"  Rank:\", best_model._java_obj.parent().getRank())\n",
        "# # Print \"MaxIter\"\n",
        "# print(\"  MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
        "# # Print \"RegParam\"\n",
        "# print(\"  RegParam:\", best_model._java_obj.parent().getRegParam())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAQY2sP_HGYq",
        "outputId": "5ea69c28-4473-4bbf-8c19-60b646e892cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/04/12 05:50:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , customer_id, article_id, rating\n",
            " Schema: _c0, customer_id, article_id, rating\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///home/shane/Co_filter_code/co_filter/sampled_1000_user_all_purchase.csv\n",
            "22/04/12 05:50:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , customer_id, article_id, rating\n",
            " Schema: _c0, customer_id, article_id, rating\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///home/shane/Co_filter_code/co_filter/sampled_1000_user_all_purchase.csv\n",
            "22/04/12 05:50:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , customer_id, article_id, rating\n",
            " Schema: _c0, customer_id, article_id, rating\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///home/shane/Co_filter_code/co_filter/sampled_1000_user_all_purchase.csv\n",
            "22/04/12 05:50:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , customer_id, article_id, rating\n",
            " Schema: _c0, customer_id, article_id, rating\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///home/shane/Co_filter_code/co_filter/sampled_1000_user_all_purchase.csv\n",
            "/home/shane/Co_filter_code/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "(train, test) = ratingDf.randomSplit([0.8, 0.2])\n",
        "als = ALS(maxIter=5, regParam=0.01, userCol=\"customerId\", itemCol=\"articleId\", ratingCol=\"countNorm\",\n",
        "          coldStartStrategy=\"drop\")\n",
        "model = als.fit(train)\n",
        "\n",
        "# Generate n Recommendations for all users\n",
        "recommendations = model.recommendForAllUsers(12)\n",
        "pickle_dump(f\"./co_filter/recommendations_{userCount}_user.pkl\", recommendations.collect())\n",
        "\n",
        "nrecommendations = recommendations.withColumn(\"rec_exp\", explode(\"recommendations\"))\\\n",
        "    .select('customerId', col(\"rec_exp.articleId\"), col(\"rec_exp.rating\"))\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "w = Window.partitionBy(nrecommendations.customerId).orderBy(nrecommendations.rating)\n",
        "rec_ranked = nrecommendations.withColumn(\"rank\", rank().over(w))\n",
        "rec_list = rec_ranked.groupBy(\"customerId\") .agg(collect_list(\"articleId\").alias(\"articleList\"))\n",
        "# rec_list.show(5)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "IpiDH7bYUZvd",
        "outputId": "c20c2774-237f-4c6a-e12c-92a17c74cfd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "import pickle\n",
        "def pickle_load(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "\n",
        "rec_dict = rec_list.toPandas()\n",
        "customerIdDecode = pickle_load(f\"./co_filter/customer_id_decoding_{userCount}_user.pkl\")\n",
        "articleIdDecode = pickle_load(f\"./co_filter/article_id_decoding_{userCount}_user.pkl\")\n",
        "\n",
        "rec_dict.replace({\"customerId\": customerIdDecode}, inplace=True)\n",
        "rec_dict[\"articleList\"] = rec_dict[\"articleList\"].apply(lambda x: list(itemgetter(*x)(articleIdDecode)))\n",
        "\n",
        "def write_formatted(df, fp):\n",
        "    df[\"stringTemp\"] = df[\"articleList\"].apply(lambda x: str(x)[1:-1].replace(',', \"\"))\n",
        "    df[[\"customerId\", \"stringTemp\"]].to_csv(fp, header=[\"customer_id\",\"prediction\"], index=None, sep=',', mode='w+')\n",
        "    \n",
        "write_formatted(rec_dict, f\"./result{userCount}user.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "pySparkCoFilter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
